{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Ensemble_Classifier.ipynb","provenance":[{"file_id":"1WgeakwrrNqAGzeKMKaKzrE_ppHOnT8mx","timestamp":1634364668765}],"collapsed_sections":[],"authorship_tag":"ABX9TyO1B6zpQJOsK/ErA6reZegM"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ecdml6euczJK","executionInfo":{"status":"ok","timestamp":1634455194556,"user_tz":-120,"elapsed":234,"user":{"displayName":"AI Vengers","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06452332470317445956"}},"outputId":"c54a5626-3413-4eaf-8476-9fa490248698"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"]}]},{"cell_type":"code","metadata":{"id":"MEJvhz1XqOkm","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1634455216713,"user_tz":-120,"elapsed":21922,"user":{"displayName":"AI Vengers","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06452332470317445956"}},"outputId":"a0bec87e-645a-4c60-b70e-55585db14f80"},"source":["#Importing Libraries\n","!pip3 install graphviz\n","!pip3 install dask\n","!pip install \"dask[complete]\" \n","!pip3 install toolz\n","!pip3 install cloudpickle\n","!pip install scikit-learn -U\n","# https://www.youtube.com/watch?v=ieW3G7ZzRZ0\n","# https://github.com/dask/dask-tutorial\n","import dask.dataframe as dd#similar to pandas\n","\n","import pandas as pd#pandas to create small dataframes \n","\n","# if this doesnt work refere install_folium.JPG in drive\n","import folium #open street map\n","\n","# unix time: https://www.unixtimestamp.com/\n","import datetime #Convert to unix time\n","\n","import time #Convert to unix time\n","\n","# if numpy is not installed already : pip3 install numpy\n","import numpy as np#Do aritmetic operations on arrays\n","\n","# matplotlib: used to plot graphs\n","import matplotlib\n","# matplotlib.use('nbagg') : matplotlib uses this protocall which makes plots more user intractive like zoom in and zoom out\n","matplotlib.use('nbagg')\n","import matplotlib.pylab as plt\n","import seaborn as sns#Plots\n","from matplotlib import rcParams#Size of plots  \n","\n","# this lib is used while we calculate the stight line distance between two (lat,lon) pairs in miles\n","!pip install gpxpy\n","import gpxpy.geo #Get the haversine distance\n","\n","from sklearn.cluster import MiniBatchKMeans, KMeans#Clustering\n","import math\n","import pickle\n","import os\n","\n","# download migwin: https://mingw-w64.org/doku.php/download/mingw-builds\n","# install it in your system and keep the path, migw_path ='installed path'\n","mingw_path = 'C:\\Program Files (x86)\\mingw-w64\\i686-8.1.0-posix-dwarf-rt_v6-rev0\\mingw32\\bin'\n","os.environ['PATH'] = mingw_path + ';' + os.environ['PATH']\n","\n","# to install xgboost: pip3 install xgboost\n","# if it didnt happen check install_xgboost.JPG\n","import xgboost as xgb\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","import datetime\n","import time\n","from IPython.display import display\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: graphviz in /usr/local/lib/python3.7/dist-packages (0.10.1)\n","Requirement already satisfied: dask in /usr/local/lib/python3.7/dist-packages (2.12.0)\n","Requirement already satisfied: dask[complete] in /usr/local/lib/python3.7/dist-packages (2.12.0)\n","Requirement already satisfied: pandas>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from dask[complete]) (1.1.5)\n","Requirement already satisfied: cloudpickle>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from dask[complete]) (2.0.0)\n","Requirement already satisfied: numpy>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from dask[complete]) (1.19.5)\n","Requirement already satisfied: bokeh>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from dask[complete]) (2.3.3)\n","Requirement already satisfied: toolz>=0.7.3 in /usr/local/lib/python3.7/dist-packages (from dask[complete]) (0.11.1)\n","Requirement already satisfied: PyYaml in /usr/local/lib/python3.7/dist-packages (from dask[complete]) (3.13)\n","Requirement already satisfied: fsspec>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from dask[complete]) (2021.10.1)\n","Requirement already satisfied: distributed>=2.0 in /usr/local/lib/python3.7/dist-packages (from dask[complete]) (2.30.1)\n","Requirement already satisfied: partd>=0.3.10 in /usr/local/lib/python3.7/dist-packages (from dask[complete]) (1.2.0)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from bokeh>=1.0.0->dask[complete]) (2.8.2)\n","Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from bokeh>=1.0.0->dask[complete]) (3.7.4.3)\n","Requirement already satisfied: tornado>=5.1 in /usr/local/lib/python3.7/dist-packages (from bokeh>=1.0.0->dask[complete]) (5.1.1)\n","Requirement already satisfied: pillow>=7.1.0 in /usr/local/lib/python3.7/dist-packages (from bokeh>=1.0.0->dask[complete]) (7.1.2)\n","Requirement already satisfied: Jinja2>=2.9 in /usr/local/lib/python3.7/dist-packages (from bokeh>=1.0.0->dask[complete]) (2.11.3)\n","Requirement already satisfied: packaging>=16.8 in /usr/local/lib/python3.7/dist-packages (from bokeh>=1.0.0->dask[complete]) (21.0)\n","Requirement already satisfied: sortedcontainers!=2.0.0,!=2.0.1 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.0->dask[complete]) (2.4.0)\n","Requirement already satisfied: psutil>=5.0 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.0->dask[complete]) (5.4.8)\n","Requirement already satisfied: msgpack>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.0->dask[complete]) (1.0.2)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from distributed>=2.0->dask[complete]) (57.4.0)\n","Requirement already satisfied: zict>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.0->dask[complete]) (2.0.0)\n","Requirement already satisfied: click>=6.6 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.0->dask[complete]) (7.1.2)\n","Requirement already satisfied: tblib>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.0->dask[complete]) (1.7.0)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2>=2.9->bokeh>=1.0.0->dask[complete]) (2.0.1)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=16.8->bokeh>=1.0.0->dask[complete]) (2.4.7)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23.0->dask[complete]) (2018.9)\n","Requirement already satisfied: locket in /usr/local/lib/python3.7/dist-packages (from partd>=0.3.10->dask[complete]) (0.2.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->bokeh>=1.0.0->dask[complete]) (1.15.0)\n","Requirement already satisfied: heapdict in /usr/local/lib/python3.7/dist-packages (from zict>=0.1.3->distributed>=2.0->dask[complete]) (1.0.1)\n","Requirement already satisfied: toolz in /usr/local/lib/python3.7/dist-packages (0.11.1)\n","Requirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (2.0.0)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (1.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (3.0.0)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.0.1)\n","Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.4.1)\n","Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.19.5)\n","Requirement already satisfied: gpxpy in /usr/local/lib/python3.7/dist-packages (1.4.2)\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NuL1dDixEuEI","executionInfo":{"status":"ok","timestamp":1634455216714,"user_tz":-120,"elapsed":23,"user":{"displayName":"AI Vengers","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06452332470317445956"}},"outputId":"759d40d7-40d5-40be-96b9-32aed6247144"},"source":["%pylab inline"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Populating the interactive namespace from numpy and matplotlib\n"]}]},{"cell_type":"markdown","metadata":{"id":"4qRfaur50AOG"},"source":["## Clustering"]},{"cell_type":"code","metadata":{"id":"elcLIH1jDBW7"},"source":["def unix2datetime(unx):\n","  return datetime.datetime.utcfromtimestamp(int(unx))\n","\n","def datetime2unix(dt):\n","  return time.mktime(dt.timetuple())\n","\n","def make_clusters(df, n_clusters):\n","  coords = df[['pickup_latitude', 'pickup_longitude']].values\n","  kmns = MiniBatchKMeans(n_clusters=n_clusters, batch_size=10000,random_state=0).fit(coords)\n","  df['pickup_cluster'] = kmns.predict(coords)\n","  return kmns\n","\n","\n","def make_bins(df, bin_mins):\n","  min_unix = df['pickup_times'].min()\n","  min_dt = unix2datetime(min_unix)\n","  year, month = min_dt.year, min_dt.month\n","  start_dt = datetime.date(year, month, 1)\n","  start_unix = datetime2unix(start_dt)\n","\n","  period = bin_mins * 60\n","  bins = []\n","  for pu in df['pickup_times'].values:\n","    bins.append(int((pu - start_unix) // period))\n","\n","  df['pickup_bins'] = bins"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Wy2dEeRBG50o"},"source":["def visualize_cluster_centers_map(centers):\n","  cluster_len = len(cluster_centers)\n","  map_osm = folium.Map(location=[40.734695, -73.990372], tiles='Stamen Toner')\n","  for i in range(cluster_len):\n","      folium.Marker(list((cluster_centers[i][0],cluster_centers[i][1])), popup=(str(cluster_centers[i][0])+str(cluster_centers[i][1]))).add_to(map_osm)\n","  display(map_osm)\n","\n","def plot_clusters(df):\n","  city_long_border = (-74.03, -73.75)\n","  city_lat_border = (40.63, 40.85)\n","  fig, ax = plt.subplots(figsize=(15,15),ncols=1, nrows=1)\n","  ax.scatter(df.pickup_longitude.values[:100000], df.pickup_latitude.values[:100000], s=10, lw=0,\n","                c=df.pickup_cluster.values[:100000], cmap='tab20', alpha=0.2)\n","  ax.set_xlim(city_long_border)\n","  ax.set_ylim(city_lat_border)\n","  ax.set_xlabel('Longitude')\n","  ax.set_ylabel('Latitude')\n","  plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QTYNAvpjI4xb"},"source":["def df2numpy(df, num_clusters):\n","  num_bins = df['pickup_bins'].max() + 1\n","  data = np.zeros((num_bins,num_clusters))\n","  for i in range(num_bins):\n","    cur_bin = df[df['pickup_bins'] == i]['pickup_cluster']\n","    for pu in cur_bin:\n","      data[i][pu] += 1\n","  return data\n","\n","def get_name(year, month, num_clusters, bin_mins):\n","  if month <= 10:\n","    month = \"0\" + str(month)\n","  name = \"{}-{}_{}_{}.npy\".format(year, month, num_clusters, bin_mins)\n","  return name\n","\n","def save_data(data, year, month, num_clusters, bin_mins, save_dir=\"/content/gdrive/MyDrive/New_York_Data/Clustered/\"):\n","  name = get_name(year, month, num_clusters, bin_mins)\n","  save_path = os.path.join(save_dir, name)\n","  if os.path.exists(save_path):\n","    print(\"{} already exists, skipping...\".format(save_path))\n","    return save_path\n","  with open(save_path, \"wb\") as f:\n","    np.save(f, data)\n","  return save_path"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5K1G5gS4Kpuv"},"source":["def get_cluster_name(year, month, num_clusters, suffix=''):\n","  if month <= 10:\n","    month = \"0\" + str(month)\n","  name = \"cluster_centers_{}-{}{}_{}.npy\".format(year, month, suffix, num_clusters)\n","  return name\n","\n","def save_cluster_centers(data, year, month, num_clusters, \\\n","                         save_dir=\"/content/gdrive/MyDrive/New_York_Data/Clustered/\"):\n","  name = get_cluster_name(year, month, num_clusters)\n","  path = os.path.join(save_dir, name)\n","  if os.path.exists(path):\n","    print(\"{} already exists, skipping...\".format(path))\n","  else:\n","    with open(path, 'wb') as f:\n","      np.save(f, data, allow_pickle=True)\n","  return path\n","\n","\n","def load_cleaned_data(year, month, load_dir=\"/content/gdrive/MyDrive/New_York_Data/clean/\"):\n","  if month <= 10:\n","    month = \"0\" + str(month)\n","  name = \"clean_yellow_tripdata_{}-{}.csv\".format(year, month)\n","  path = os.path.join(load_dir, name)\n","  if not os.path.exists(path):\n","    raise Exception(\"{} does not exist...\".format(path))\n","  cleaned_data = pd.read_csv(path)\n","  return cleaned_data\n","\n","def preprocess2npy(df, year, month, num_clusters, bin_mins, save_dir=\"/content/gdrive/MyDrive/New_York_Data/Clustered/\"):\n","  print(\"Making clusters...\")\n","  kmns = make_clusters(df, num_clusters)\n","  print(\"Saving cluster centers...\")\n","  path = save_cluster_centers(kmns.cluster_centers_, year, month, num_clusters, save_dir)\n","  print(\"Cluster centers saved to {} ...\".format(path))\n","  print(\"Making bins...\")\n","  make_bins(df, bin_mins)\n","  data = df2numpy(df, num_clusters)\n","  print(\"Saving...\")\n","  save_path = save_data(data, year, month, num_clusters, bin_mins, save_dir)\n","  print(\"Saved to\", save_path)\n","  return save_path"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TTEfH-RcKx7x"},"source":["cleaned_data = pd.read_csv(\"/content/gdrive/MyDrive/New_York_Data/clean/clean_yellow_tripdata_2016-02.csv\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":171},"id":"x-NcwcrmLOle","executionInfo":{"status":"ok","timestamp":1634455278480,"user_tz":-120,"elapsed":54284,"user":{"displayName":"AI Vengers","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06452332470317445956"}},"outputId":"6ab43f14-5c68-43b0-febd-828c65350b7e"},"source":["preprocess2npy(cleaned_data, 2016, 2, 125, 30)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Making clusters...\n","Saving cluster centers...\n","/content/gdrive/MyDrive/New_York_Data/Clustered/cluster_centers_2016-02_125.npy already exists, skipping...\n","Cluster centers saved to /content/gdrive/MyDrive/New_York_Data/Clustered/cluster_centers_2016-02_125.npy ...\n","Making bins...\n","Saving...\n","/content/gdrive/MyDrive/New_York_Data/Clustered/2016-02_125_30.npy already exists, skipping...\n","Saved to /content/gdrive/MyDrive/New_York_Data/Clustered/2016-02_125_30.npy\n"]},{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'/content/gdrive/MyDrive/New_York_Data/Clustered/2016-02_125_30.npy'"]},"metadata":{},"execution_count":270}]},{"cell_type":"markdown","metadata":{"id":"-gcTny0idZw_"},"source":["# Preprocess data"]},{"cell_type":"code","metadata":{"id":"jsAB267fdqca"},"source":["def load_data(year, month, num_clusters, bin_mins, \\\n","              load_dir=\"/content/gdrive/MyDrive/New_York_Data/Clustered\"):\n","  name = get_name(year, month, num_clusters, bin_mins)\n","  path = os.path.join(load_dir, name)\n","  if not os.path.exists(path):\n","    raise Exception(\"Could not find {}\".format(path))\n","  \n","  with open(path, 'rb') as f:\n","    data = np.load(f, allow_pickle=True)\n","  return data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mT1gIcMffAco"},"source":["def get_weekdays(year, month, data, bin_mins):\n","  start_dt = datetime.date(year, month, 1)\n","  start_wd = start_dt.weekday()\n","  day_sec = 60 * 60 * 24\n","  week_sec = day_sec * 7\n","  weekdays = []\n","  for i in range(len(data)):\n","    secs = i * 60 * bin_mins\n","    weekday = ((secs % week_sec) // day_sec + start_wd) % 7\n","    weekdays.append(weekday)\n","  return weekdays\n","\n","def load_cluster_centers(year, month, num_clusters, suffix=\"\", \\\n","            load_dir=\"/content/gdrive/MyDrive/New_York_Data/Clustered\",\\\n","      ):\n","  name = get_cluster_name(year, month, num_clusters)\n","  path = os.path.join(load_dir, name)\n","  if not os.path.exists(path):\n","    raise Exception(\"Could not find {}\".format(path))\n","\n","  with open(path, 'rb') as f:\n","    data = np.load(f, allow_pickle=True)\n","  return data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XBJhXmpSkyiO"},"source":["def make_seq(data, cluster_centers, weekdays, daybins, window_size=10):\n","  seq_data = []\n","  num_bins = len(data)\n","  num_clusters = len(data[0])\n","  lat = cluster_centers[:, 1]\n","  lon = cluster_centers[:, 0]\n","  for i in range(num_clusters):\n","    for j in range(num_bins - window_size):\n","      seq = list(data[j:j+window_size, i])\n","      for item in [lat[i], lon[i], weekdays[j], daybins[j], data[j+window_size][i]]:\n","        seq.append(item)\n","      seq_data.append(seq)\n","\n","  return np.array(seq_data)\n","\n","def train_test_split(data, weekdays, daybins, train_ratio=0.7):\n","  train_size = int(len(data) * train_ratio)\n","  train_data = data[:train_size]\n","  test_data = data[train_size:]\n","  train_weekdays = weekdays[:train_size]\n","  test_weekdays = weekdays[train_size:]\n","  train_daybins = daybins[:train_size]\n","  test_daybins = daybins[train_size:]\n","  return train_data, test_data, train_weekdays, test_weekdays, train_daybins, test_daybins"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OcHsWJ-RpYDM"},"source":["def get_full_data(year, month, num_clusters, bin_mins, \\\n","                  load_dir=\"/content/gdrive/MyDrive/New_York_Data/Clustered\"):\n","  data = load_data(year, month, num_clusters, bin_mins)\n","  print(data.shape)\n","  cluster_centers = load_cluster_centers(year, month, num_clusters)\n","  weekdays = get_weekdays(year, month, data, bin_mins)\n","  daybins = np.mod(np.arange(len(data)),24*60//bin_mins)\n","  train_data, test_data, train_weekdays, test_weekdays, train_daybins, test_daybins = train_test_split(data, weekdays, daybins)\n","  train_data = make_seq(train_data, cluster_centers, train_weekdays, train_daybins)\n","  test_data = make_seq(test_data, cluster_centers, test_weekdays, test_daybins)\n","  x_train, y_train = train_data[:, :-1], train_data[:, -1]\n","  x_test, y_test = test_data[:, :-1], test_data[:, -1]\n","\n","  return x_train, y_train, x_test, y_test"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pzypbeNACFev"},"source":["def get_weekly_periodic_data(year, month, day, num_clusters, bin_mins, \\\n","                  load_dir=\"/content/gdrive/MyDrive/New_York_Data/Clustered\"):\n","  data = load_data(year, month, num_clusters, bin_mins)\n","  cluster_centers = load_cluster_centers(year, month, num_clusters)\n","  weekdays = get_weekdays(year, month, data, bin_mins)\n","  daybins = np.mod(np.arange(len(data)),24*60//bin_mins)\n","  daybins_ext = np.repeat(daybins[:,np.newaxis], len(cluster_centers), axis=1)\n","  lat = np.repeat(cluster_centers[np.newaxis, :, 1], len(data), axis=0)\n","  lon = np.repeat(cluster_centers[np.newaxis, :, 0], len(data), axis=0)\n","\n","  data = np.stack((daybins_ext, lat, lon, data), axis=2)\n","  train_data, test_data, train_weekdays, test_weekdays, train_daybins, test_daybins = train_test_split(data, weekdays, daybins)\n","\n","  train_data = train_data[np.array(train_weekdays)==day].reshape(-1,4)\n","  test_data = test_data[np.array(test_weekdays)==day].reshape(-1,4)\n","  \n","  x_train, y_train = train_data[:, :-1], train_data[:, -1]\n","  x_test, y_test = test_data[:, :-1], test_data[:, -1]\n","\n","  return x_train, y_train, x_test, y_test"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZROkD9hMFhTJ"},"source":["#First Decision Tree with Periodic Boundary Conditions\n","\n"]},{"cell_type":"code","metadata":{"id":"cfjHDzdRPwXH"},"source":["year = 2016\n","month = 2\n","bin_mins = 30\n","num_clusters = 125"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":171},"id":"4TzhhHt6P5Q3","executionInfo":{"status":"ok","timestamp":1634455443439,"user_tz":-120,"elapsed":59259,"user":{"displayName":"AI Vengers","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06452332470317445956"}},"outputId":"fe449134-02bc-484d-ccf7-043d72706438"},"source":["cleaned_data = load_cleaned_data(year, month)\n","preprocess2npy(cleaned_data, year, month, num_clusters, bin_mins)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Making clusters...\n","Saving cluster centers...\n","/content/gdrive/MyDrive/New_York_Data/Clustered/cluster_centers_2016-02_125.npy already exists, skipping...\n","Cluster centers saved to /content/gdrive/MyDrive/New_York_Data/Clustered/cluster_centers_2016-02_125.npy ...\n","Making bins...\n","Saving...\n","/content/gdrive/MyDrive/New_York_Data/Clustered/2016-02_125_30.npy already exists, skipping...\n","Saved to /content/gdrive/MyDrive/New_York_Data/Clustered/2016-02_125_30.npy\n"]},{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'/content/gdrive/MyDrive/New_York_Data/Clustered/2016-02_125_30.npy'"]},"metadata":{},"execution_count":289}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qlRPGzhMP-XK","executionInfo":{"status":"ok","timestamp":1634455907382,"user_tz":-120,"elapsed":5399,"user":{"displayName":"AI Vengers","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06452332470317445956"}},"outputId":"04765688-e9fd-4fcf-c911-fb059bb766cc"},"source":["from sklearn.ensemble import RandomForestRegressor\n","from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error\n","\n","def mape(y_true, y_pred):\n","  err = mean_absolute_error(y_true, y_pred) / (sum(y_true) / len(y_true))\n","  return err\n","\n","weekday_regressors = []\n","for day in range(7):\n","  x_train, y_train, x_test, y_test = get_weekly_periodic_data(year, month, day, num_clusters, bin_mins)\n","  weekday_regressors.append(RandomForestRegressor(max_features='sqrt',min_samples_leaf=9,min_samples_split=7,n_estimators=79, n_jobs=-1))\n","  weekday_regressors[day].fit(x_train, y_train)\n","\n","print(\"Reassuringly, the Sunday classifier performs significantly better on the weekend than on workdays:\")\n","for i in weekday_regressors:\n","  pred = i.predict(x_test)\n","  print(mape(pred, y_test))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Reassuringly, the Sunday classifier performs significantly better on the weekend than on workdays:\n","0.4229569494195547\n","0.496461347141585\n","0.4883327588022728\n","0.486269060603158\n","0.44653599485439893\n","0.3135395272612615\n","0.2348956080376671\n"]}]},{"cell_type":"markdown","metadata":{"id":"GSl82k_D3tTN"},"source":["# Second Decision Tree for Time-Series Forecast"]},{"cell_type":"code","metadata":{"id":"75SSJ4SSpt_j"},"source":["year = 2016\n","month = 2\n","bin_mins = 30\n","num_clusters = 125"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":171},"id":"3AYR3hsvDLTx","executionInfo":{"status":"ok","timestamp":1634455508796,"user_tz":-120,"elapsed":59739,"user":{"displayName":"AI Vengers","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06452332470317445956"}},"outputId":"9cc7af57-0519-4238-8751-62b07288f17d"},"source":["cleaned_data = load_cleaned_data(year, month)\n","preprocess2npy(cleaned_data, year, month, num_clusters, bin_mins)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Making clusters...\n","Saving cluster centers...\n","/content/gdrive/MyDrive/New_York_Data/Clustered/cluster_centers_2016-02_125.npy already exists, skipping...\n","Cluster centers saved to /content/gdrive/MyDrive/New_York_Data/Clustered/cluster_centers_2016-02_125.npy ...\n","Making bins...\n","Saving...\n","/content/gdrive/MyDrive/New_York_Data/Clustered/2016-02_125_30.npy already exists, skipping...\n","Saved to /content/gdrive/MyDrive/New_York_Data/Clustered/2016-02_125_30.npy\n"]},{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'/content/gdrive/MyDrive/New_York_Data/Clustered/2016-02_125_30.npy'"]},"metadata":{},"execution_count":292}]},{"cell_type":"code","metadata":{"id":"ZXbMIjRBDKcG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1634455553241,"user_tz":-120,"elapsed":1561,"user":{"displayName":"AI Vengers","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06452332470317445956"}},"outputId":"a4a80257-b4a8-4cfc-db44-c711200fdee3"},"source":["x_train, y_train, x_test, y_test = get_full_data(year, month, num_clusters, bin_mins)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(1392, 125)\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TIl74CmM9CbC","executionInfo":{"status":"ok","timestamp":1634455557734,"user_tz":-120,"elapsed":3094,"user":{"displayName":"AI Vengers","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06452332470317445956"}},"outputId":"95d1a843-799f-40e3-8ab2-f0da28f42136"},"source":["print(np.abs(y_train).mean())\n","\n","predictions = []\n","for r in weekday_regressors:\n","  predictions.append(r.predict( x_train[:, [-1,-4,-3]]) )\n","\n","for i in range(len(x_train)):\n","  weekday = int(x_train[i][-2])\n","  y_train[i] -= predictions[weekday][i]\n","\n","print(np.abs(y_train).mean())"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["64.11177593360996\n","42.12251445116694\n"]}]},{"cell_type":"code","metadata":{"id":"U0eC8qVBHj5t"},"source":["from sklearn.linear_model import LinearRegression\n","from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lj6L-ktYzBNj"},"source":["def mape(y_true, y_pred):\n","  err = mean_absolute_error(y_true, y_pred) / (sum(y_true) / len(y_true))\n","  return err"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QjSVexigHw2Z"},"source":["from sklearn.ensemble import RandomForestClassifier\n","\n","regr1 = RandomForestRegressor(max_features='sqrt',min_samples_leaf=9,min_samples_split=7,n_estimators=100, n_jobs=-1)\n","regr1.fit(x_train, y_train)\n","\n","predictions = []\n","for r in weekday_regressors:\n","  predictions.append(r.predict( x_test[:, [-1,-4,-3]]) )\n","\n","y_pred = np.zeros(len(x_test))\n","reg_pred = regr1.predict(x_test)\n","\n","for i in range(len(x_test)):\n","  weekday = int(x_test[i][-2])\n","  y_pred[i] = predictions[weekday][i] + reg_pred[i]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DAfcIxW-U-rX","executionInfo":{"status":"ok","timestamp":1634455640430,"user_tz":-120,"elapsed":9,"user":{"displayName":"AI Vengers","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06452332470317445956"}},"outputId":"168f3a6f-4978-4ad8-d1ac-cc97532c87b2"},"source":["mape(y_test, y_pred)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.20686466532085795"]},"metadata":{},"execution_count":307}]},{"cell_type":"code","metadata":{"id":"W7PNuuFcIfDo"},"source":[""],"execution_count":null,"outputs":[]}]}