{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Taxi_Prediction_GCNN.ipynb","provenance":[{"file_id":"1WgeakwrrNqAGzeKMKaKzrE_ppHOnT8mx","timestamp":1634364668765}],"collapsed_sections":[],"authorship_tag":"ABX9TyPt5wSB0DT4yc8J69cGOnyu"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ecdml6euczJK","executionInfo":{"status":"ok","timestamp":1634432944544,"user_tz":-120,"elapsed":563,"user":{"displayName":"AI Vengers","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06452332470317445956"}},"outputId":"ce1a803c-ea01-4ca1-aa3b-7aa706b16c02"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"]}]},{"cell_type":"code","metadata":{"id":"MEJvhz1XqOkm","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1634432613167,"user_tz":-120,"elapsed":22742,"user":{"displayName":"AI Vengers","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06452332470317445956"}},"outputId":"49d122b4-a7a6-497e-f1b2-adb53278bded"},"source":["#Importing Libraries\n","!pip3 install graphviz\n","!pip3 install dask\n","!pip install \"dask[complete]\" \n","!pip3 install toolz\n","!pip3 install cloudpickle\n","# https://www.youtube.com/watch?v=ieW3G7ZzRZ0\n","# https://github.com/dask/dask-tutorial\n","# please do go through this python notebook: https://github.com/dask/dask-tutorial/blob/master/07_dataframe.ipynb\n","import dask.dataframe as dd#similar to pandas\n","\n","import pandas as pd#pandas to create small dataframes \n","\n","# if this doesnt work refere install_folium.JPG in drive\n","import folium #open street map\n","\n","# unix time: https://www.unixtimestamp.com/\n","import datetime #Convert to unix time\n","\n","import time #Convert to unix time\n","\n","# if numpy is not installed already : pip3 install numpy\n","import numpy as np#Do aritmetic operations on arrays\n","\n","# matplotlib: used to plot graphs\n","import matplotlib\n","# matplotlib.use('nbagg') : matplotlib uses this protocall which makes plots more user intractive like zoom in and zoom out\n","matplotlib.use('nbagg')\n","import matplotlib.pylab as plt\n","import seaborn as sns#Plots\n","from matplotlib import rcParams#Size of plots  \n","\n","# this lib is used while we calculate the stight line distance between two (lat,lon) pairs in miles\n","!pip install gpxpy\n","import gpxpy.geo #Get the haversine distance\n","\n","from sklearn.cluster import MiniBatchKMeans, KMeans#Clustering\n","import math\n","import pickle\n","import os\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","import datetime\n","import time\n","from IPython.display import display\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: graphviz in /usr/local/lib/python3.7/dist-packages (0.10.1)\n","Requirement already satisfied: dask in /usr/local/lib/python3.7/dist-packages (2.12.0)\n","Requirement already satisfied: dask[complete] in /usr/local/lib/python3.7/dist-packages (2.12.0)\n","Requirement already satisfied: cloudpickle>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from dask[complete]) (1.3.0)\n","Collecting partd>=0.3.10\n","  Downloading partd-1.2.0-py3-none-any.whl (19 kB)\n","Requirement already satisfied: numpy>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from dask[complete]) (1.19.5)\n","Collecting fsspec>=0.6.0\n","  Downloading fsspec-2021.10.1-py3-none-any.whl (125 kB)\n","\u001b[K     |████████████████████████████████| 125 kB 13.4 MB/s \n","\u001b[?25hCollecting distributed>=2.0\n","  Downloading distributed-2021.9.1-py3-none-any.whl (786 kB)\n","\u001b[K     |████████████████████████████████| 786 kB 51.1 MB/s \n","\u001b[?25hRequirement already satisfied: pandas>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from dask[complete]) (1.1.5)\n","Requirement already satisfied: toolz>=0.7.3 in /usr/local/lib/python3.7/dist-packages (from dask[complete]) (0.11.1)\n","Requirement already satisfied: bokeh>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from dask[complete]) (2.3.3)\n","Requirement already satisfied: PyYaml in /usr/local/lib/python3.7/dist-packages (from dask[complete]) (3.13)\n","Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from bokeh>=1.0.0->dask[complete]) (3.7.4.3)\n","Requirement already satisfied: packaging>=16.8 in /usr/local/lib/python3.7/dist-packages (from bokeh>=1.0.0->dask[complete]) (21.0)\n","Requirement already satisfied: tornado>=5.1 in /usr/local/lib/python3.7/dist-packages (from bokeh>=1.0.0->dask[complete]) (5.1.1)\n","Requirement already satisfied: Jinja2>=2.9 in /usr/local/lib/python3.7/dist-packages (from bokeh>=1.0.0->dask[complete]) (2.11.3)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from bokeh>=1.0.0->dask[complete]) (2.8.2)\n","Requirement already satisfied: pillow>=7.1.0 in /usr/local/lib/python3.7/dist-packages (from bokeh>=1.0.0->dask[complete]) (7.1.2)\n","Requirement already satisfied: zict>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.0->dask[complete]) (2.0.0)\n","Requirement already satisfied: tblib>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.0->dask[complete]) (1.7.0)\n","Requirement already satisfied: psutil>=5.0 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.0->dask[complete]) (5.4.8)\n","Collecting cloudpickle>=0.2.1\n","  Downloading cloudpickle-2.0.0-py3-none-any.whl (25 kB)\n","Requirement already satisfied: sortedcontainers!=2.0.0,!=2.0.1 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.0->dask[complete]) (2.4.0)\n","Requirement already satisfied: msgpack>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.0->dask[complete]) (1.0.2)\n","Requirement already satisfied: click>=6.6 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.0->dask[complete]) (7.1.2)\n","Collecting distributed>=2.0\n","  Downloading distributed-2021.9.0-py3-none-any.whl (779 kB)\n","\u001b[K     |████████████████████████████████| 779 kB 54.3 MB/s \n","\u001b[?25h  Downloading distributed-2021.8.1-py3-none-any.whl (778 kB)\n","\u001b[K     |████████████████████████████████| 778 kB 65.2 MB/s \n","\u001b[?25h  Downloading distributed-2021.8.0-py3-none-any.whl (776 kB)\n","\u001b[K     |████████████████████████████████| 776 kB 48.7 MB/s \n","\u001b[?25h  Downloading distributed-2021.7.2-py3-none-any.whl (769 kB)\n","\u001b[K     |████████████████████████████████| 769 kB 54.3 MB/s \n","\u001b[?25h  Downloading distributed-2021.7.1-py3-none-any.whl (766 kB)\n","\u001b[K     |████████████████████████████████| 766 kB 53.7 MB/s \n","\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from distributed>=2.0->dask[complete]) (57.4.0)\n","  Downloading distributed-2021.7.0-py3-none-any.whl (1.0 MB)\n","\u001b[K     |████████████████████████████████| 1.0 MB 48.4 MB/s \n","\u001b[?25h  Downloading distributed-2021.6.2-py3-none-any.whl (722 kB)\n","\u001b[K     |████████████████████████████████| 722 kB 48.3 MB/s \n","\u001b[?25h  Downloading distributed-2021.6.1-py3-none-any.whl (722 kB)\n","\u001b[K     |████████████████████████████████| 722 kB 49.8 MB/s \n","\u001b[?25h  Downloading distributed-2021.6.0-py3-none-any.whl (715 kB)\n","\u001b[K     |████████████████████████████████| 715 kB 52.4 MB/s \n","\u001b[?25h  Downloading distributed-2021.5.1-py3-none-any.whl (705 kB)\n","\u001b[K     |████████████████████████████████| 705 kB 33.4 MB/s \n","\u001b[?25h  Downloading distributed-2021.5.0-py3-none-any.whl (699 kB)\n","\u001b[K     |████████████████████████████████| 699 kB 48.0 MB/s \n","\u001b[?25h  Downloading distributed-2021.4.1-py3-none-any.whl (696 kB)\n","\u001b[K     |████████████████████████████████| 696 kB 43.8 MB/s \n","\u001b[?25h  Downloading distributed-2021.4.0-py3-none-any.whl (684 kB)\n","\u001b[K     |████████████████████████████████| 684 kB 51.1 MB/s \n","\u001b[?25h  Downloading distributed-2021.3.1-py3-none-any.whl (679 kB)\n","\u001b[K     |████████████████████████████████| 679 kB 43.2 MB/s \n","\u001b[?25h  Downloading distributed-2021.3.0-py3-none-any.whl (675 kB)\n","\u001b[K     |████████████████████████████████| 675 kB 50.5 MB/s \n","\u001b[?25h  Downloading distributed-2021.2.0-py3-none-any.whl (675 kB)\n","\u001b[K     |████████████████████████████████| 675 kB 46.6 MB/s \n","\u001b[?25h  Downloading distributed-2021.1.1-py3-none-any.whl (672 kB)\n","\u001b[K     |████████████████████████████████| 672 kB 60.9 MB/s \n","\u001b[?25h  Downloading distributed-2021.1.0-py3-none-any.whl (671 kB)\n","\u001b[K     |████████████████████████████████| 671 kB 61.7 MB/s \n","\u001b[?25h  Downloading distributed-2020.12.0-py3-none-any.whl (669 kB)\n","\u001b[K     |████████████████████████████████| 669 kB 40.2 MB/s \n","\u001b[?25h  Downloading distributed-2.30.1-py3-none-any.whl (656 kB)\n","\u001b[K     |████████████████████████████████| 656 kB 43.3 MB/s \n","\u001b[?25hRequirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2>=2.9->bokeh>=1.0.0->dask[complete]) (2.0.1)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=16.8->bokeh>=1.0.0->dask[complete]) (2.4.7)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23.0->dask[complete]) (2018.9)\n","Collecting locket\n","  Downloading locket-0.2.1-py2.py3-none-any.whl (4.1 kB)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->bokeh>=1.0.0->dask[complete]) (1.15.0)\n","Requirement already satisfied: heapdict in /usr/local/lib/python3.7/dist-packages (from zict>=0.1.3->distributed>=2.0->dask[complete]) (1.0.1)\n","Installing collected packages: locket, cloudpickle, partd, fsspec, distributed\n","  Attempting uninstall: cloudpickle\n","    Found existing installation: cloudpickle 1.3.0\n","    Uninstalling cloudpickle-1.3.0:\n","      Successfully uninstalled cloudpickle-1.3.0\n","  Attempting uninstall: distributed\n","    Found existing installation: distributed 1.25.3\n","    Uninstalling distributed-1.25.3:\n","      Successfully uninstalled distributed-1.25.3\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","gym 0.17.3 requires cloudpickle<1.7.0,>=1.2.0, but you have cloudpickle 2.0.0 which is incompatible.\u001b[0m\n","Successfully installed cloudpickle-2.0.0 distributed-2.30.1 fsspec-2021.10.1 locket-0.2.1 partd-1.2.0\n","Requirement already satisfied: toolz in /usr/local/lib/python3.7/dist-packages (0.11.1)\n","Requirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (2.0.0)\n","Collecting gpxpy\n","  Downloading gpxpy-1.4.2.tar.gz (105 kB)\n","\u001b[K     |████████████████████████████████| 105 kB 15.1 MB/s \n","\u001b[?25hBuilding wheels for collected packages: gpxpy\n","  Building wheel for gpxpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for gpxpy: filename=gpxpy-1.4.2-py3-none-any.whl size=42562 sha256=42948fbc4983f697d1fd0ba76392cdbf7148b8ad03048151ed0da7215954dc63\n","  Stored in directory: /root/.cache/pip/wheels/e9/1b/e8/1e95d95fb1af470b278323a5564f4508f64c2aa476e4547f63\n","Successfully built gpxpy\n","Installing collected packages: gpxpy\n","Successfully installed gpxpy-1.4.2\n"]}]},{"cell_type":"code","metadata":{"id":"dQH0cfZZNUbk"},"source":["cleaned_data = pd.read_csv(r'/content/gdrive/MyDrive/New_York_Data/clean/clean_yellow_tripdata_2016-01_first_week.csv')#, assume_missing=True)#.iloc[:, 1:4]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NuL1dDixEuEI","executionInfo":{"status":"ok","timestamp":1634432617889,"user_tz":-120,"elapsed":24,"user":{"displayName":"AI Vengers","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06452332470317445956"}},"outputId":"a94ac431-51d2-4613-d161-2f9f7c2eac05"},"source":["%pylab inline"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Populating the interactive namespace from numpy and matplotlib\n"]}]},{"cell_type":"markdown","metadata":{"id":"4qRfaur50AOG"},"source":["## Function Wrappers for Data Preprocessing / Clustering / Visualization\n","\n","(See the main notebook \"Taxi_Prediction.ipynb\" for comments on this part)"]},{"cell_type":"code","metadata":{"id":"elcLIH1jDBW7"},"source":["def unix2datetime(unx):\n","  return datetime.datetime.utcfromtimestamp(int(unx))\n","\n","def datetime2unix(dt):\n","  return time.mktime(dt.timetuple())\n","\n","def make_clusters(df, n_clusters):\n","  coords = df[['pickup_latitude', 'pickup_longitude']].values\n","  kmns = MiniBatchKMeans(n_clusters=n_clusters, batch_size=10000,random_state=0).fit(coords)\n","  df['pickup_cluster'] = kmns.predict(coords)\n","  return kmns\n","\n","\n","def make_bins(df, bin_mins):\n","  min_unix = df['pickup_times'].min()\n","  min_dt = unix2datetime(min_unix)\n","  year, month = min_dt.year, min_dt.month\n","  start_dt = datetime.date(year, month, 1)\n","  start_unix = datetime2unix(start_dt)\n","\n","  period = bin_mins * 60\n","  bins = []\n","  for pu in df['pickup_times'].values:\n","    bins.append(int((pu - start_unix) // period))\n","\n","  df['pickup_bins'] = bins"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Wy2dEeRBG50o"},"source":["def visualize_cluster_centers_map(centers):\n","  cluster_len = len(cluster_centers)\n","  map_osm = folium.Map(location=[40.734695, -73.990372], tiles='Stamen Toner')\n","  for i in range(cluster_len):\n","      folium.Marker(list((cluster_centers[i][0],cluster_centers[i][1])), popup=(str(cluster_centers[i][0])+str(cluster_centers[i][1]))).add_to(map_osm)\n","  display(map_osm)\n","\n","def plot_clusters(df):\n","  city_long_border = (-74.03, -73.75)\n","  city_lat_border = (40.63, 40.85)\n","  fig, ax = plt.subplots(figsize=(15,15),ncols=1, nrows=1)\n","  ax.scatter(df.pickup_longitude.values[:100000], df.pickup_latitude.values[:100000], s=10, lw=0,\n","                c=df.pickup_cluster.values[:100000], cmap='tab20', alpha=0.2)\n","  ax.set_xlim(city_long_border)\n","  ax.set_ylim(city_lat_border)\n","  ax.set_xlabel('Longitude')\n","  ax.set_ylabel('Latitude')\n","  plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QTYNAvpjI4xb"},"source":["def df2numpy(df, num_clusters):\n","  num_bins = df['pickup_bins'].max() + 1\n","  data = np.zeros((num_bins,num_clusters))\n","  for i in range(num_bins):\n","    cur_bin = df[df['pickup_bins'] == i]['pickup_cluster']\n","    for pu in cur_bin:\n","      data[i][pu] += 1\n","  return data\n","\n","def save_data(data, year, month, num_clusters, bin_mins, save_dir=\"/content/gdrive/MyDrive/New_York_Data/Clustered/\"):\n","  if month <= 10:\n","    month = \"0\" + str(month)\n","  name = \"{}-{}_{}_{}.npy\".format(year, month, num_clusters, bin_mins)\n","  save_path = os.path.join(save_dir, name)\n","  if os.path.exists(save_path):\n","    print(\"{} already exists, skipping...\".format(save_path))\n","    return save_path\n","  with open(save_path, \"wb\") as f:\n","    np.save(f, data)\n","  return save_path"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5K1G5gS4Kpuv"},"source":["def preprocess2npy(df, year, month, num_clusters, bin_mins, save_dir=\"/content/gdrive/MyDrive/New_York_Data/Clustered/\"):\n","  print(\"Making clusters...\")\n","  kmns = make_clusters(df, num_clusters)\n","  print(\"Making bins...\")\n","  make_bins(df, bin_mins)\n","  data = df2numpy(df, num_clusters)\n","  print(\"Saving...\")\n","  save_path = save_data(data, year, month, num_clusters, bin_mins, save_dir)\n","  print(\"Saved to\", save_path)\n","  return save_path"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TTEfH-RcKx7x"},"source":["cleaned_data = pd.read_csv(r'/content/gdrive/MyDrive/New_York_Data/clean/clean_yellow_tripdata_2016-01_first_week.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":103},"id":"x-NcwcrmLOle","executionInfo":{"status":"ok","timestamp":1634434197299,"user_tz":-120,"elapsed":86717,"user":{"displayName":"AI Vengers","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06452332470317445956"}},"outputId":"b4c0b585-1b1a-4dac-bbf2-3b9baa70119f"},"source":["preprocess2npy(cleaned_data, 2016, 1, 1000, 60)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Making clusters...\n","Making bins...\n","Saving...\n","Saved to /content/gdrive/MyDrive/New_York_Data/Clustered/2016-01_1000_60.npy\n"]},{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'/content/gdrive/MyDrive/New_York_Data/Clustered/2016-01_1000_60.npy'"]},"metadata":{},"execution_count":55}]},{"cell_type":"code","metadata":{"id":"1d5EuzDfQswz"},"source":["kmns = make_clusters(cleaned_data, 1000)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oQUPVmC7Q3tE"},"source":["with open(\"/content/gdrive/MyDrive/New_York_Data/Clustered/cluster_centers_2016-01_first_week.npy\", \"wb\") as f:\n","  np.save(f, kmns.cluster_centers_)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0mLJzQeScH-_"},"source":["##Install Torch-Geometric Dependencies"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6j8dMkYBI077","executionInfo":{"status":"ok","timestamp":1634432730395,"user_tz":-120,"elapsed":21020,"user":{"displayName":"AI Vengers","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06452332470317445956"}},"outputId":"a97c7272-ca61-4f7d-ab11-5eb1558ec58e"},"source":["!pip install torch-scatter -f https://data.pyg.org/whl/torch-1.9.0+cu111.html\n","!pip install torch-sparse -f https://data.pyg.org/whl/torch-1.9.0+cu111.html\n","!pip install torch-geometric\n","!pip install torch-cluster -f https://data.pyg.org/whl/torch-1.9.0+cu111.html\n","!pip install torch-spline-conv -f https://data.pyg.org/whl/torch-1.9.0+cu111.html"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in links: https://data.pyg.org/whl/torch-1.9.0+cu111.html\n","Collecting torch-scatter\n","  Downloading https://data.pyg.org/whl/torch-1.9.0%2Bcu111/torch_scatter-2.0.8-cp37-cp37m-linux_x86_64.whl (10.4 MB)\n","\u001b[K     |████████████████████████████████| 10.4 MB 13.6 MB/s \n","\u001b[?25hInstalling collected packages: torch-scatter\n","Successfully installed torch-scatter-2.0.8\n","Looking in links: https://data.pyg.org/whl/torch-1.9.0+cu111.html\n","Collecting torch-sparse\n","  Downloading https://data.pyg.org/whl/torch-1.9.0%2Bcu111/torch_sparse-0.6.12-cp37-cp37m-linux_x86_64.whl (3.7 MB)\n","\u001b[K     |████████████████████████████████| 3.7 MB 10.9 MB/s \n","\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch-sparse) (1.4.1)\n","Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scipy->torch-sparse) (1.19.5)\n","Installing collected packages: torch-sparse\n","Successfully installed torch-sparse-0.6.12\n","Collecting torch-geometric\n","  Downloading torch_geometric-2.0.1.tar.gz (308 kB)\n","\u001b[K     |████████████████████████████████| 308 kB 16.3 MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.19.5)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (4.62.3)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.4.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.6.3)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (0.22.2.post1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.23.0)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.1.5)\n","Collecting rdflib\n","  Downloading rdflib-6.0.2-py3-none-any.whl (407 kB)\n","\u001b[K     |████████████████████████████████| 407 kB 43.3 MB/s \n","\u001b[?25hRequirement already satisfied: googledrivedownloader in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (0.4)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.11.3)\n","Requirement already satisfied: pyparsing in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.4.7)\n","Collecting yacs\n","  Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (3.13)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->torch-geometric) (2.0.1)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->torch-geometric) (2.8.2)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->torch-geometric) (2018.9)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->torch-geometric) (1.15.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from rdflib->torch-geometric) (57.4.0)\n","Collecting isodate\n","  Downloading isodate-0.6.0-py2.py3-none-any.whl (45 kB)\n","\u001b[K     |████████████████████████████████| 45 kB 4.0 MB/s \n","\u001b[?25hRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (2021.5.30)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch-geometric) (1.0.1)\n","Building wheels for collected packages: torch-geometric\n","  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for torch-geometric: filename=torch_geometric-2.0.1-py3-none-any.whl size=513822 sha256=a1de788a2ac54996fdf959924832a41f96b4da52c2fd215e9d376cab2e999e04\n","  Stored in directory: /root/.cache/pip/wheels/78/3d/42/20589db73c66b5109fb93a0c5743edfd6ab5ca820a52afacfc\n","Successfully built torch-geometric\n","Installing collected packages: isodate, yacs, rdflib, torch-geometric\n","Successfully installed isodate-0.6.0 rdflib-6.0.2 torch-geometric-2.0.1 yacs-0.1.8\n","Looking in links: https://data.pyg.org/whl/torch-1.9.0+cu111.html\n","Collecting torch-cluster\n","  Downloading https://data.pyg.org/whl/torch-1.9.0%2Bcu111/torch_cluster-1.5.9-cp37-cp37m-linux_x86_64.whl (1.6 MB)\n","\u001b[K     |████████████████████████████████| 1.6 MB 3.2 MB/s \n","\u001b[?25hInstalling collected packages: torch-cluster\n","Successfully installed torch-cluster-1.5.9\n","Looking in links: https://data.pyg.org/whl/torch-1.9.0+cu111.html\n","Collecting torch-spline-conv\n","  Downloading https://data.pyg.org/whl/torch-1.9.0%2Bcu111/torch_spline_conv-1.2.1-cp37-cp37m-linux_x86_64.whl (870 kB)\n","\u001b[K     |████████████████████████████████| 870 kB 14.8 MB/s \n","\u001b[?25hInstalling collected packages: torch-spline-conv\n","Successfully installed torch-spline-conv-1.2.1\n"]}]},{"cell_type":"markdown","metadata":{"id":"DZBgqOdecTWC"},"source":["## Defining the G-CNN "]},{"cell_type":"code","metadata":{"id":"QZqtxTJ7Ilpl"},"source":["import torch\n","from torch_geometric.data import Data, InMemoryDataset, DataLoader\n","from torch_geometric.nn import radius_graph\n","from torch_geometric.utils import add_self_loops\n","from sklearn.metrics.pairwise import rbf_kernel\n","\n","class TaxiGraphDataset(InMemoryDataset):\n","  def __init__(self, device, transform=None, pre_transform=None):\n","    self.window_size = 5\n","    self.shuffle_dataset = False\n","    super(TaxiGraphDataset, self).__init__(\"/content/gdrive/MyDrive/New_York_Data/Clustered\", transform, pre_transform)\n","    self.data, self.slices = torch.load(self.processed_paths[0])\n","    self.data.to(device)\n","\n","  @property\n","  def raw_file_names(self):\n","    return [\"/content/gdrive/MyDrive/New_York_Data/Clustered/2016-01_1000_60.npy\"]\n","\n","  @property\n","  def processed_file_names(self):\n","    return ['data.pt']\n","\n","  def download(self):\n","    pass\n","\n","  def process(self):\n","    with open(\"/content/gdrive/MyDrive/New_York_Data/Clustered/2016-01_1000_60.npy\", 'rb') as f:\n","      data_list = []\n","      np_data = np.load(f)\n","      #print(shape(np_data))\n","      np_data = np.stack((np.roll(np_data, i, axis=0) for i in range(-1,self.window_size)), axis=2)\n","      #print(shape(np_data))\n","      indices = list(range(np_data.shape[0]))\n","      if self.shuffle_dataset:\n","        np.random.seed(self.random_seed)\n","        np.random.shuffle(indices)\n","\n","    with open(\"/content/gdrive/MyDrive/New_York_Data/Clustered/cluster_centers_2016-01_first_week.npy\", \"rb\") as f:\n","      cluster_centers = np.load(f, allow_pickle=True)\n","\n","      clusterpos_x = cluster_centers[:,0] - np.mean(cluster_centers[:,0])\n","      clusterpos_y = cluster_centers[:,1] - np.mean(cluster_centers[:,1])\n","\n","      clusterpos_x /= np.std(clusterpos_x)\n","      clusterpos_y /= np.std(clusterpos_y)\n","      clusterpos = np.stack((clusterpos_x, clusterpos_y), axis=1)\n","\n","      position_embedding = np.repeat(rbf_kernel(clusterpos, gamma=1000)[np.newaxis,:,:], len(np_data), axis=0)\n","\n","      #clusterpos_x = np.repeat(np.expand_dims((clusterpos_x), axis=(0,2)), 1009, axis=0)\n","      #clusterpos_y = np.repeat(np.expand_dims((clusterpos_y), axis=(0,2)), 1009, axis=0)\n","      #print(shape(clusterpos_x))\n","      data = np.concatenate((np_data, 50*position_embedding), axis=2)\n","      #print(shape(data))\n","\n","    for i in indices:\n","      pos = torch.tensor(clusterpos)\n","      edges = radius_graph(x=pos, r=0.05)\n","      print(edges.shape)\n","      #edges, _ = add_self_loops(edges, num_nodes=pos.size(0))\n","      data_list.append(Data(edge_index=edges, x=torch.tensor(data[i], dtype=torch.float32)))\n","      \n","    if self.pre_filter is not None:\n","      data_list = [data for data in data_list if self.pre_filter(data)]\n","\n","    if self.pre_transform is not None:\n","      data_list = [self.pre_transform(data) for data in data_list]\n","\n","    data, slices = self.collate(data_list)\n","    torch.save((data, slices), self.processed_paths[0])   "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Z-LGxp8W_ut4"},"source":[""]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dQTvKSAMyCZQ","executionInfo":{"status":"ok","timestamp":1634435685156,"user_tz":-120,"elapsed":7441,"user":{"displayName":"AI Vengers","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06452332470317445956"}},"outputId":"0e56e466-1a89-484a-ddca-094b4deb4108"},"source":["from torch.utils.data.sampler import SubsetRandomSampler\n","def train_test_split(ds, train_split=0.8, batch_size=4):\n","  dataset_size = len(ds)\n","  indices = list(range(dataset_size))\n","  index_split = int(train_split * dataset_size)\n","  train_indices, test_indices = indices[:index_split], indices[index_split:]\n","  train_sampler = SubsetRandomSampler(train_indices)\n","  test_sampler = SubsetRandomSampler(test_indices)\n","\n","  train_loader = DataLoader(ds, batch_size=batch_size,\n","                            sampler=train_sampler)\n","  test_loader = DataLoader(ds, batch_size=batch_size,\n","                            sampler=test_sampler)\n","  \n","  return (train_loader, test_loader)\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(device)\n","\n","ds = TaxiGraphDataset(device)\n","train_loader, test_loader = train_test_split(ds, train_split=0.8)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["cpu\n"]},{"output_type":"stream","name":"stderr","text":["Processing...\n","/usr/local/lib/python3.7/dist-packages/torch_geometric/data/dataset.py:88: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n","  self._process()\n"]},{"output_type":"stream","name":"stdout","text":["torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n","torch.Size([2, 1588])\n"]},{"output_type":"stream","name":"stderr","text":["Done!\n","/usr/local/lib/python3.7/dist-packages/torch_geometric/deprecation.py:13: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n","  warnings.warn(out)\n"]}]},{"cell_type":"code","metadata":{"id":"Kl3BALTky4hT"},"source":["from torch_geometric.nn import GCNConv\n","import torch.nn.functional as F\n","\n","class GCN(torch.nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.conv1 = GCNConv(1005, 1)\n","        #for p in self.conv1.parameters():\n","          #torch.nn.init.zeros_(p.data)\n","        #self.conv2 = GCNConv(32, 1)\n","        #self.conv3 = GCNConv(30, 1)\n","        #for p in self.conv2.parameters():\n","          #torch.nn.init.zeros_(p.data)\n","\n","    def forward(self, data):\n","        x, edge_index = data.x[:,1:], data.edge_index\n","        #print(x)\n","        x = self.conv1(x, edge_index)\n","        x = F.relu(x)\n","        #x = F.dropout(x, training=self.training)\n","        x = self.conv2(x, edge_index)\n","        x = F.relu(x)\n","        #x = self.conv3(x, edge_index)\n","        #x = F.relu(x)\n","        return x[:, 0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MFusHvlG9MMl"},"source":["model = GCN().to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"alXNU5sK-5v5","executionInfo":{"status":"error","timestamp":1634435972525,"user_tz":-120,"elapsed":281278,"user":{"displayName":"AI Vengers","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06452332470317445956"}},"outputId":"609d36ae-d7be-4ab4-fe70-b6c129ab8dab"},"source":["optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-3)\n","\n","#model.train()\n","for epoch in range(200):\n","  for batch in train_loader:\n","    #print(batch.edge_index)\n","    #print(\"x\")\n","    #print(batch.x[:,0])\n","    #print(batch.x)\n","    y = batch.x[:,0]\n","    #print(\"y\")\n","    #print(y)\n","    pred = model(batch)\n","    #print(\"y: \", y)\n","    #print(y.dtype)\n","    #print(\"x: \", batch.x)\n","    #print(\"pred: \", pred)\n","    #print(torch.abs((pred - y) / (y+0.5)))\n","    #mape_loss = torch.mean(torch.abs((pred - y) / (y+1e-6) ) )  \n","    loss =  F.mse_loss(pred, y)\n","    optimizer.zero_grad()\n","    loss.backward()\n","    optimizer.step()\n","  \n","  losses = []\n","  for batch in test_loader:\n","    y = batch.x[:,0]\n","    pred = model(batch)\n","    #print(y)\n","    #print(pred)\n","    mape = err_metric(y.cpu().detach().numpy(), pred.cpu().detach().numpy())\n","    losses.append(mape.item())\n","\n","    #print(losses)\n","\n","  test_mape_loss =  np.array(losses).mean()\n","  print(\"Epoch: \", epoch, \"MAPE Loss: \", test_mape_loss)\n","    #for p in model.parameters():\n","    #print(p.name, p.data)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch:  0 MAPE Loss:  0.6514700591089748\n","Epoch:  1 MAPE Loss:  1.8281544705769934\n","Epoch:  2 MAPE Loss:  0.5143207929532819\n","Epoch:  3 MAPE Loss:  0.48627718526587915\n","Epoch:  4 MAPE Loss:  0.4995847202727713\n","Epoch:  5 MAPE Loss:  0.45312332418855916\n","Epoch:  6 MAPE Loss:  0.45777787873634473\n","Epoch:  7 MAPE Loss:  0.4653333363602048\n","Epoch:  8 MAPE Loss:  0.4638445983134301\n","Epoch:  9 MAPE Loss:  0.46138657639474306\n","Epoch:  10 MAPE Loss:  0.4563547676603614\n","Epoch:  11 MAPE Loss:  0.4552096655377596\n","Epoch:  12 MAPE Loss:  0.4584844044395677\n","Epoch:  13 MAPE Loss:  0.43576014243986555\n","Epoch:  14 MAPE Loss:  0.45110332034549405\n","Epoch:  15 MAPE Loss:  0.45157574497132047\n","Epoch:  16 MAPE Loss:  0.4507356727877252\n","Epoch:  17 MAPE Loss:  0.4468139488515799\n","Epoch:  18 MAPE Loss:  0.43013035572588504\n","Epoch:  19 MAPE Loss:  0.4430583351944366\n","Epoch:  20 MAPE Loss:  0.437923111748019\n","Epoch:  21 MAPE Loss:  0.44170510333854796\n","Epoch:  22 MAPE Loss:  0.4503388301040174\n","Epoch:  23 MAPE Loss:  0.4790162755710109\n","Epoch:  24 MAPE Loss:  0.4320906674086842\n","Epoch:  25 MAPE Loss:  0.4307177434264073\n","Epoch:  26 MAPE Loss:  0.4680568107648748\n","Epoch:  27 MAPE Loss:  0.45018682195633886\n","Epoch:  28 MAPE Loss:  0.4620508291972994\n","Epoch:  29 MAPE Loss:  0.4388689025978955\n","Epoch:  30 MAPE Loss:  0.43456579645941307\n","Epoch:  31 MAPE Loss:  0.4410784314457001\n","Epoch:  32 MAPE Loss:  0.4265849547245027\n","Epoch:  33 MAPE Loss:  0.4267222976217336\n","Epoch:  34 MAPE Loss:  0.4229562425656765\n","Epoch:  35 MAPE Loss:  0.4216292448310821\n","Epoch:  36 MAPE Loss:  0.42246225671153637\n","Epoch:  37 MAPE Loss:  0.4323714189904886\n","Epoch:  38 MAPE Loss:  0.42783988272840423\n","Epoch:  39 MAPE Loss:  0.43373061121214707\n","Epoch:  40 MAPE Loss:  0.4411978926764913\n","Epoch:  41 MAPE Loss:  0.4564530895781339\n","Epoch:  42 MAPE Loss:  0.45203926346280165\n","Epoch:  43 MAPE Loss:  0.45458332709030924\n","Epoch:  44 MAPE Loss:  0.42419505985198547\n","Epoch:  45 MAPE Loss:  0.5701110964266781\n","Epoch:  46 MAPE Loss:  0.4460312170504459\n","Epoch:  47 MAPE Loss:  0.42846118457363513\n","Epoch:  48 MAPE Loss:  0.43613159132930973\n","Epoch:  49 MAPE Loss:  0.42653253997238444\n","Epoch:  50 MAPE Loss:  0.44255177912419547\n","Epoch:  51 MAPE Loss:  0.42293342295590874\n","Epoch:  52 MAPE Loss:  0.4396810104221037\n","Epoch:  53 MAPE Loss:  0.4168364745650148\n","Epoch:  54 MAPE Loss:  0.4171287416315494\n","Epoch:  55 MAPE Loss:  0.42000526871291455\n","Epoch:  56 MAPE Loss:  0.4248566579703031\n","Epoch:  57 MAPE Loss:  0.4186580119537287\n","Epoch:  58 MAPE Loss:  0.44304227889798753\n","Epoch:  59 MAPE Loss:  0.43131765533139443\n","Epoch:  60 MAPE Loss:  0.43427260502929943\n","Epoch:  61 MAPE Loss:  0.4186118344596892\n","Epoch:  62 MAPE Loss:  0.4171248550586825\n","Epoch:  63 MAPE Loss:  0.41787844229570026\n","Epoch:  64 MAPE Loss:  0.4534753896753709\n","Epoch:  65 MAPE Loss:  0.42011854117151376\n","Epoch:  66 MAPE Loss:  0.44544057806757237\n","Epoch:  67 MAPE Loss:  0.452257290791243\n","Epoch:  68 MAPE Loss:  0.4241900179715999\n","Epoch:  69 MAPE Loss:  0.4680317893534249\n","Epoch:  70 MAPE Loss:  0.4377161393730673\n","Epoch:  71 MAPE Loss:  0.4378708512531623\n","Epoch:  72 MAPE Loss:  0.44332546873518397\n","Epoch:  73 MAPE Loss:  0.44139370075152384\n","Epoch:  74 MAPE Loss:  0.4166995411951539\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-55-441811205009>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    147\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"V0KhWys_b44R"},"source":["from sklearn.metrics import mean_absolute_error\n","def err_metric(y_true, y_pred):\n","  err = mean_absolute_error(y_true, y_pred) / (np.sum(y_true) / len(y_true))\n","  return err"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WnhHBNbZF1EL","executionInfo":{"status":"ok","timestamp":1634435496547,"user_tz":-120,"elapsed":406,"user":{"displayName":"AI Vengers","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06452332470317445956"}},"outputId":"f9e49206-8747-4eee-9228-d057870d2092"},"source":["losses = []\n","for batch in test_loader:\n","  y = batch.x[:,0]\n","  pred = model(batch)\n","  #print(y)\n","  #print(pred)\n","  mape = err_metric(y.cpu().detach().numpy(), pred.cpu().detach().numpy())\n","  losses.append(mape.item())\n","\n","#print(losses)\n","\n","test_mse_loss =  np.array(losses).mean()\n","print(test_mse_loss)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0.41249619189620945\n"]}]},{"cell_type":"markdown","metadata":{"id":"ICodCD4yGqCQ"},"source":[""]}]}