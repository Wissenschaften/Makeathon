{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Taxi_Prediction.ipynb","provenance":[{"file_id":"1WgeakwrrNqAGzeKMKaKzrE_ppHOnT8mx","timestamp":1634364668765}],"collapsed_sections":["ZJprsbWw3qGh"],"authorship_tag":"ABX9TyOle3LD3dC5FkwxokIHNqvj"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ecdml6euczJK","executionInfo":{"status":"ok","timestamp":1634441596544,"user_tz":-240,"elapsed":18982,"user":{"displayName":"AI Vengers","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06452332470317445956"}},"outputId":"82732351-0ae5-4f1a-9a32-d7a59adf43dd"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}]},{"cell_type":"code","metadata":{"id":"MEJvhz1XqOkm"},"source":["#Importing Libraries\n","!pip3 install graphviz\n","!pip3 install dask\n","!pip install \"dask[complete]\" \n","!pip3 install toolz\n","!pip3 install cloudpickle\n","!pip install scikit-learn -U\n","# https://www.youtube.com/watch?v=ieW3G7ZzRZ0\n","# https://github.com/dask/dask-tutorial\n","# please do go through this python notebook: https://github.com/dask/dask-tutorial/blob/master/07_dataframe.ipynb\n","import dask.dataframe as dd#similar to pandas\n","\n","import pandas as pd#pandas to create small dataframes \n","\n","# if this doesnt work refere install_folium.JPG in drive\n","import folium #open street map\n","\n","# unix time: https://www.unixtimestamp.com/\n","import datetime #Convert to unix time\n","\n","import time #Convert to unix time\n","\n","# if numpy is not installed already : pip3 install numpy\n","import numpy as np#Do aritmetic operations on arrays\n","\n","# matplotlib: used to plot graphs\n","import matplotlib\n","# matplotlib.use('nbagg') : matplotlib uses this protocall which makes plots more user intractive like zoom in and zoom out\n","matplotlib.use('nbagg')\n","import matplotlib.pylab as plt\n","import seaborn as sns#Plots\n","from matplotlib import rcParams#Size of plots  \n","import json\n","\n","# this lib is used while we calculate the stight line distance between two (lat,lon) pairs in miles\n","!pip install gpxpy\n","import gpxpy.geo #Get the haversine distance\n","\n","from sklearn.cluster import MiniBatchKMeans, KMeans#Clustering\n","import math\n","import pickle\n","import os\n","from IPython.display import display\n","\n","# download migwin: https://mingw-w64.org/doku.php/download/mingw-builds\n","# install it in your system and keep the path, migw_path ='installed path'\n","mingw_path = 'C:\\Program Files (x86)\\mingw-w64\\i686-8.1.0-posix-dwarf-rt_v6-rev0\\mingw32\\bin'\n","os.environ['PATH'] = mingw_path + ';' + os.environ['PATH']\n","\n","# to install xgboost: pip3 install xgboost\n","# if it didnt happen check install_xgboost.JPG\n","import xgboost as xgb\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4qRfaur50AOG"},"source":["## Clustering\n","\n","Here we do the clustering and save the resulting data as a .npy file"]},{"cell_type":"code","metadata":{"id":"elcLIH1jDBW7"},"source":["def unix2datetime(unx):\n","  return datetime.datetime.utcfromtimestamp(int(unx))\n","\n","def datetime2unix(dt):\n","  return time.mktime(dt.timetuple())\n","\n","def make_clusters(df, n_clusters):\n","  coords = df[['pickup_latitude', 'pickup_longitude']].values\n","  kmns = MiniBatchKMeans(n_clusters=n_clusters, batch_size=10000,random_state=0).fit(coords)\n","  df['pickup_cluster'] = kmns.predict(coords)\n","  return kmns\n","\n","\n","def make_bins(df, bin_mins):\n","  min_unix = df['pickup_times'].min()\n","  min_dt = unix2datetime(min_unix)\n","  year, month = min_dt.year, min_dt.month\n","  start_dt = datetime.date(year, month, 1)\n","  start_unix = datetime2unix(start_dt)\n","\n","  period = bin_mins * 60\n","  bins = []\n","  for pu in df['pickup_times'].values:\n","    bins.append(int((pu - start_unix) // period))\n","\n","  df['pickup_bins'] = bins"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Wy2dEeRBG50o"},"source":["def visualize_cluster_centers_map(centers):\n","  cluster_len = len(cluster_centers)\n","  map_osm = folium.Map(location=[40.734695, -73.990372], tiles='Stamen Toner')\n","  for i in range(cluster_len):\n","      folium.Marker(list((cluster_centers[i][0],cluster_centers[i][1])), popup=(str(cluster_centers[i][0])+str(cluster_centers[i][1]))).add_to(map_osm)\n","  display(map_osm)\n","\n","def plot_clusters(df):\n","  city_long_border = (-74.03, -73.75)\n","  city_lat_border = (40.63, 40.85)\n","  fig, ax = plt.subplots(figsize=(15,15),ncols=1, nrows=1)\n","  ax.scatter(df.pickup_longitude.values[:100000], df.pickup_latitude.values[:100000], s=10, lw=0,\n","                c=df.pickup_cluster.values[:100000], cmap='tab20', alpha=0.2)\n","  ax.set_xlim(city_long_border)\n","  ax.set_ylim(city_lat_border)\n","  ax.set_xlabel('Longitude')\n","  ax.set_ylabel('Latitude')\n","  plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QTYNAvpjI4xb"},"source":["def df2numpy(df, num_clusters):\n","  num_bins = df['pickup_bins'].max() + 1\n","  data = np.zeros((num_bins,num_clusters))\n","  for i in range(num_bins):\n","    cur_bin = df[df['pickup_bins'] == i]['pickup_cluster']\n","    for pu in cur_bin:\n","      data[i][pu] += 1\n","  return data\n","\n","def get_name(year, month, num_clusters, bin_mins):\n","  if month <= 10:\n","    month = \"0\" + str(month)\n","  name = \"{}-{}_{}_{}.npy\".format(year, month, num_clusters, bin_mins)\n","  return name\n","\n","def save_data(data, year, month, num_clusters, bin_mins, save_dir=\"/content/gdrive/MyDrive/New_York_Data/Clustered/\"):\n","  name = get_name(year, month, num_clusters, bin_mins)\n","  save_path = os.path.join(save_dir, name)\n","  if os.path.exists(save_path):\n","    print(\"{} already exists, skipping...\".format(save_path))\n","    return save_path\n","  with open(save_path, \"wb\") as f:\n","    np.save(f, data)\n","  return save_path"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5K1G5gS4Kpuv"},"source":["def get_cluster_name(year, month, num_clusters, suffix=''):\n","  if month <= 10:\n","    month = \"0\" + str(month)\n","  name = \"cluster_centers_{}-{}{}_{}.npy\".format(year, month, suffix, num_clusters)\n","  return name\n","\n","def save_cluster_centers(data, year, month, num_clusters, \\\n","                         save_dir=\"/content/gdrive/MyDrive/New_York_Data/Clustered/\"):\n","  name = get_cluster_name(year, month, num_clusters)\n","  path = os.path.join(save_dir, name)\n","  if os.path.exists(path):\n","    print(\"{} already exists, skipping...\".format(path))\n","  else:\n","    with open(path, 'wb') as f:\n","      np.save(f, data, allow_pickle=True)\n","  return path\n","\n","def get_cluster_grid_name(year, month, grid_resolution, suffix=''):\n","  if month <= 10:\n","    month = \"0\" + str(month)\n","  name = \"cluster_grid_{}-{}{}_{}.npy\".format(year, month, suffix, grid_resolution)\n","  return name\n","\n","def save_cluster_grid(\n","    kmns,\n","    grid_resolution=200,\n","    grid_corners=((40.5774, -74.15), (40.9176,-73.7004)),\n","    save_dir=\"/content/gdrive/MyDrive/New_York_Data/Clustered/\"\n","    ):\n","  \n","  latitudes = np.linspace(grid_corners[0][0], grid_corners[1][0], grid_resolution)\n","  longitudes = np.linspace(grid_corners[0][1], grid_corners[1][1], grid_resolution)\n","  \n","  X, Y = np.meshgrid(latitudes, longitudes)\n","  predicted = kmns.predict(np.vstack([X.flatten(), Y.flatten()]).T)\n","  grid = np.stack((predicted.reshape((grid_resolution, grid_resolution)), X, Y), axis=2)\n","  name = get_cluster_grid_name(year, month, grid_resolution)\n","  path = os.path.join(save_dir, name)\n","  if os.path.exists(path):\n","    print(\"{} already exists, skipping...\".format(path))\n","  else:\n","    with open(path, 'wb') as f:\n","      np.save(f, grid, allow_pickle=True)\n","  return path\n","  \n","\n","def load_cleaned_data(year, month, load_dir=\"/content/gdrive/MyDrive/New_York_Data/clean/\"):\n","  if month <= 10:\n","    month = \"0\" + str(month)\n","  name = \"clean_yellow_tripdata_{}-{}.csv\".format(year, month)\n","  path = os.path.join(load_dir, name)\n","  if not os.path.exists(path):\n","    raise Exception(\"{} does not exist...\".format(path))\n","  cleaned_data = pd.read_csv(path)\n","  return cleaned_data\n","\n","def preprocess2npy(df, year, month, num_clusters, bin_mins, save_dir=\"/content/gdrive/MyDrive/New_York_Data/Clustered/\"):\n","  print(\"Making clusters...\")\n","  kmns = make_clusters(df, num_clusters)\n","  print(\"Saving cluster centers...\")\n","  path = save_cluster_centers(kmns.cluster_centers_, year, month, num_clusters, save_dir)\n","  print(\"Cluster centers saved to {} ...\".format(path))\n","  print(\"Computing cluster grid...\")\n","  path = save_cluster_grid(kmns, 200)\n","  print(\"Cluster grid saved to {} ...\".format(path))\n","  print(\"Making bins...\")\n","  make_bins(df, bin_mins)\n","  data = df2numpy(df, num_clusters)\n","  print(\"Saving...\")\n","  save_path = save_data(data, year, month, num_clusters, bin_mins, save_dir)\n","  print(\"Saved to\", save_path)\n","  return save_path"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TTEfH-RcKx7x"},"source":["cleaned_data = pd.read_csv(\"/content/gdrive/MyDrive/New_York_Data/clean/clean_yellow_tripdata_2016-02.csv\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":209},"id":"x-NcwcrmLOle","executionInfo":{"status":"ok","timestamp":1634420731996,"user_tz":-120,"elapsed":50735,"user":{"displayName":"AI Vengers","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06452332470317445956"}},"outputId":"0fc7adb3-d0f9-40e8-e9ba-47525eed5297"},"source":["preprocess2npy(cleaned_data, 2016, 2, 125, 30)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Making clusters...\n","Saving cluster centers...\n","/content/gdrive/MyDrive/New_York_Data/Clustered/cluster_centers_2016-02_125.npy already exists, skipping...\n","Cluster centers saved to /content/gdrive/MyDrive/New_York_Data/Clustered/cluster_centers_2016-02_125.npy ...\n","Computing cluster grid...\n","Cluster grid saved to /content/gdrive/MyDrive/New_York_Data/Clustered/cluster_centers_2016-02_125.npy ...\n","Making bins...\n","Saving...\n","/content/gdrive/MyDrive/New_York_Data/Clustered/2016-02_125_30.npy already exists, skipping...\n","Saved to /content/gdrive/MyDrive/New_York_Data/Clustered/2016-02_125_30.npy\n"]},{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'/content/gdrive/MyDrive/New_York_Data/Clustered/2016-02_125_30.npy'"]},"metadata":{},"execution_count":84}]},{"cell_type":"markdown","metadata":{"id":"ZJprsbWw3qGh"},"source":["# LSTM\n","\n","We attempted at using LSTM for time-series analysis. This attempt got aborted due to time limitations"]},{"cell_type":"code","metadata":{"id":"jACxPef5WR5X"},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","\n","torch.manual_seed(1)\n","\n","from sklearn.preprocessing import MinMaxScaler"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_dIa-T1eN5eV"},"source":["def split_data(data, window_size=6):\n","  seq_data = []\n","  row, col = data.shape\n","  for i in range(0, row - window_size):\n","    inp = data[i:i+window_size]\n","    targ = data[window_size]\n","    seq_data.append((inp, targ))\n","  return seq_data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HDmLcTArecdK"},"source":["train_size = int(len(data) * 0.8)\n","train_data = data[:train_size]\n","test_data = data[train_size:]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BpLU7yMle5y8"},"source":["scaler = MinMaxScaler()\n","train_norm = scaler.fit_transform(train_data)\n","test_norm = scaler.transform(test_data)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IsV44CbrffyZ"},"source":["train_norm = torch.FloatTensor(train_norm)\n","test_norm = torch.FloatTensor(test_norm)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JK_RhfCNUYqH"},"source":["seq_data = split_data(train_norm)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"76uR4PISo8i8"},"source":["class LSTM(nn.Module):\n","\n","  def __init__(self, input_size, hidden_layer_size, output_size):\n","    super().__init__()\n","    self.hidden_layer_size = hidden_layer_size\n","    self.lstm = nn.LSTM(input_size, hidden_layer_size, batch_first=True)\n","    self.linear = nn.Linear(hidden_layer_size, output_size)\n","    self.hidden_cell = (torch.zeros(1, 1, self.output_size), torch.zeros(1, 1, self.hidden_dim))\n","\n","  def forward(self, x):\n","    lstm_out, self.hidden_cell = self.lstm(x, self.hidden_cell)\n","    predictions = self.linear()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AI3qi28qWpWM"},"source":["for inp, targ in seq_data:\n","  y_pred = model(inp)\n","  loss = "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-gcTny0idZw_"},"source":["# Preprocess data\n","\n","Here, data is preprocessed to make it ready for training. Each sample in the training data has following features:\n","\n","- Number of pickups at t-5\n","- Number of pickups at t-4\n","- Number of pickups at t-3\n","- Number of pickups at t-2\n","- Number of pickups at t-1\n","- Longitude\n","- Latitude\n","- Weekday\n","\n","The output is the number of pickups at t"]},{"cell_type":"code","metadata":{"id":"jsAB267fdqca"},"source":["def load_data(year, month, num_clusters, bin_mins, \\\n","              load_dir=\"/content/gdrive/MyDrive/New_York_Data/Clustered\"):\n","  name = get_name(year, month, num_clusters, bin_mins)\n","  path = os.path.join(load_dir, name)\n","  if not os.path.exists(path):\n","    raise Exception(\"Could not find {}\".format(path))\n","  \n","  with open(path, 'rb') as f:\n","    data = np.load(f, allow_pickle=True)\n","  return data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mT1gIcMffAco"},"source":["def get_weekdays(year, month, data, bin_mins):\n","  start_dt = datetime.date(year, month, 1)\n","  start_wd = start_dt.weekday()\n","  day_sec = 60 * 60 * 24\n","  week_sec = day_sec * 7\n","  weekdays = []\n","  for i in range(len(data)):\n","    secs = i * 60 * bin_mins\n","    weekday = ((secs % week_sec) // day_sec + start_wd) % 7\n","    weekdays.append(weekday)\n","  return weekdays\n","\n","def load_cluster_centers(year, month, num_clusters, suffix=\"\", \\\n","            load_dir=\"/content/gdrive/MyDrive/New_York_Data/Clustered\",\\\n","      ):\n","  name = get_cluster_name(year, month, num_clusters)\n","  path = os.path.join(load_dir, name)\n","  if not os.path.exists(path):\n","    raise Exception(\"Could not find {}\".format(path))\n","\n","  with open(path, 'rb') as f:\n","    data = np.load(f, allow_pickle=True)\n","  return data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XBJhXmpSkyiO"},"source":["def full_data(data, cluster_centers, weekdays):\n","  full_data = []\n","  num_bins = len(data)\n","  num_clusters = len(data[0])\n","  lat = cluster_centers[:, 1]\n","  lon = cluster_centers[:, 0]\n","    \n","\n","def make_seq(data, cluster_centers, weekdays, window_size=5):\n","  seq_data = []\n","  num_bins = len(data)\n","  num_clusters = len(data[0])\n","  lat = cluster_centers[:, 1]\n","  lon = cluster_centers[:, 0]\n","  for i in range(num_clusters):\n","    for j in range(num_bins - window_size):\n","      seq = list(data[j:j+window_size, i])\n","      for item in [lat[i], lon[i], weekdays[j], data[j+window_size][i]]:\n","        seq.append(item)\n","      seq_data.append(seq)\n","\n","  return np.array(seq_data)\n","\n","def train_test_split(data, weekdays, train_ratio=0.7):\n","  train_size = int(len(data) * train_ratio)\n","  train_data = data[:train_size]\n","  test_data = data[train_size:]\n","  train_weekdays = weekdays[:train_size]\n","  test_weekdays = weekdays[train_size:]\n","  return train_data, test_data, train_weekdays, test_weekdays"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-Tp61m6FmT_x"},"source":["data = load_data(year, month, num_clusters, bin_mins)\n","cluster_centers = load_cluster_centers(year, month, num_clusters)\n","weekdays = get_weekdays(year, month, data, bin_mins)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2vCaMCGpm1l4"},"source":["train_data, test_data, train_weekdays, test_weekdays = train_test_split(data, weekdays)\n","train_data = make_seq(train_data, cluster_centers, train_weekdays)\n","test_data = make_seq(test_data, cluster_centers, test_weekdays)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WflYwB4SIWpB"},"source":["dummy = []\n","for i, ct in enumerate(data[-1]):\n","  lat = cluster_centers[i][0]\n","  lon = cluster_centers[i][1]\n","  dummy.append([lat, lon, str(int(ct))])\n","\n","file_name = \"dummy_{}_{}.json\".format(num_clusters, bin_mins)\n","\n","with open(file_name, \"w\") as f:\n","  json.dump(dummy, f)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MaOUqD-JpKPP"},"source":["x_train, y_train = train_data[:, :-1], train_data[:, -1]\n","x_test, y_test = test_data[:, :-1], test_data[:, -1]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OcHsWJ-RpYDM"},"source":["def get_full_data(year, month, num_clusters, bin_mins, \\\n","                  load_dir=\"/content/gdrive/MyDrive/New_York_Data/Clustered\"):\n","  data = load_data(year, month, num_clusters, bin_mins)\n","  cluster_centers = load_cluster_centers(year, month, num_clusters)\n","  weekdays = get_weekdays(year, month, data, bin_mins)\n","  train_data, test_data, train_weekdays, test_weekdays = train_test_split(data, weekdays)\n","  train_data = make_seq(train_data, cluster_centers, train_weekdays)\n","  test_data = make_seq(test_data, cluster_centers, test_weekdays)\n","  x_train, y_train = train_data[:, :-1], train_data[:, -1]\n","  x_test, y_test = test_data[:, :-1], test_data[:, -1]\n","\n","  return x_train, y_train, x_test, y_test"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GSl82k_D3tTN"},"source":["# Linear Regression"]},{"cell_type":"code","metadata":{"id":"75SSJ4SSpt_j"},"source":["year = 2016\n","month = 2\n","bin_mins = 60\n","num_clusters = 1000"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":209},"id":"3AYR3hsvDLTx","executionInfo":{"status":"ok","timestamp":1634425358201,"user_tz":-120,"elapsed":104597,"user":{"displayName":"AI Vengers","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06452332470317445956"}},"outputId":"0cf73c49-d651-4757-9c33-12a18f9eb762"},"source":["cleaned_data = load_cleaned_data(year, month)\n","preprocess2npy(cleaned_data, year, month, num_clusters, bin_mins)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Making clusters...\n","Saving cluster centers...\n","/content/gdrive/MyDrive/New_York_Data/Clustered/cluster_centers_2016-02_1000.npy already exists, skipping...\n","Cluster centers saved to /content/gdrive/MyDrive/New_York_Data/Clustered/cluster_centers_2016-02_1000.npy ...\n","Computing cluster grid...\n","/content/gdrive/MyDrive/New_York_Data/Clustered/cluster_grid_2016-02_200.npy already exists, skipping...\n","Cluster grid saved to /content/gdrive/MyDrive/New_York_Data/Clustered/cluster_centers_2016-02_1000.npy ...\n","Making bins...\n","Saving...\n","Saved to /content/gdrive/MyDrive/New_York_Data/Clustered/2016-02_1000_60.npy\n"]},{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'/content/gdrive/MyDrive/New_York_Data/Clustered/2016-02_1000_60.npy'"]},"metadata":{},"execution_count":100}]},{"cell_type":"code","metadata":{"id":"ZXbMIjRBDKcG"},"source":["x_train, y_train, x_test, y_test = get_full_data(year, month, num_clusters, bin_mins)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zMi6Sl8Op-ms","executionInfo":{"status":"ok","timestamp":1634425362790,"user_tz":-120,"elapsed":29,"user":{"displayName":"AI Vengers","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06452332470317445956"}},"outputId":"014fe597-1e25-412d-b246-ff9c2f004b96"},"source":["x_train[:15], y_train[:15]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(array([[ 10.        ,   6.        ,   4.        ,   3.        ,\n","           1.        , -73.97436789,  40.75065413,   0.        ],\n","        [  6.        ,   4.        ,   3.        ,   1.        ,\n","           3.        , -73.97436789,  40.75065413,   0.        ],\n","        [  4.        ,   3.        ,   1.        ,   3.        ,\n","           9.        , -73.97436789,  40.75065413,   0.        ],\n","        [  3.        ,   1.        ,   3.        ,   9.        ,\n","          20.        , -73.97436789,  40.75065413,   0.        ],\n","        [  1.        ,   3.        ,   9.        ,  20.        ,\n","          33.        , -73.97436789,  40.75065413,   0.        ],\n","        [  3.        ,   9.        ,  20.        ,  33.        ,\n","          36.        , -73.97436789,  40.75065413,   0.        ],\n","        [  9.        ,  20.        ,  33.        ,  36.        ,\n","          35.        , -73.97436789,  40.75065413,   0.        ],\n","        [ 20.        ,  33.        ,  36.        ,  35.        ,\n","          29.        , -73.97436789,  40.75065413,   0.        ],\n","        [ 33.        ,  36.        ,  35.        ,  29.        ,\n","          32.        , -73.97436789,  40.75065413,   0.        ],\n","        [ 36.        ,  35.        ,  29.        ,  32.        ,\n","          36.        , -73.97436789,  40.75065413,   0.        ],\n","        [ 35.        ,  29.        ,  32.        ,  36.        ,\n","          23.        , -73.97436789,  40.75065413,   0.        ],\n","        [ 29.        ,  32.        ,  36.        ,  23.        ,\n","          48.        , -73.97436789,  40.75065413,   0.        ],\n","        [ 32.        ,  36.        ,  23.        ,  48.        ,\n","          43.        , -73.97436789,  40.75065413,   0.        ],\n","        [ 36.        ,  23.        ,  48.        ,  43.        ,\n","          67.        , -73.97436789,  40.75065413,   0.        ],\n","        [ 23.        ,  48.        ,  43.        ,  67.        ,\n","          62.        , -73.97436789,  40.75065413,   0.        ]]),\n"," array([ 3.,  9., 20., 33., 36., 35., 29., 32., 36., 23., 48., 43., 67.,\n","        62., 72.]))"]},"metadata":{},"execution_count":102}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OYM3N_Ap33rt","executionInfo":{"status":"ok","timestamp":1634425362790,"user_tz":-120,"elapsed":19,"user":{"displayName":"AI Vengers","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06452332470317445956"}},"outputId":"86e4bc31-d5bc-4617-965e-758188a237ea"},"source":["y_train.mean()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["16.02794398340249"]},"metadata":{},"execution_count":103}]},{"cell_type":"code","metadata":{"id":"U0eC8qVBHj5t"},"source":["from sklearn.linear_model import LinearRegression\n","from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lj6L-ktYzBNj"},"source":["def mape(y_true, y_pred):\n","  err = mean_absolute_error(y_true, y_pred) / (sum(y_true) / len(y_true))\n","  return err"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QjSVexigHw2Z"},"source":["lr = LinearRegression().fit(x_train, y_train)\n","pred = lr.predict(x_test)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DAfcIxW-U-rX","executionInfo":{"status":"ok","timestamp":1634425363150,"user_tz":-120,"elapsed":369,"user":{"displayName":"AI Vengers","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06452332470317445956"}},"outputId":"c8cc6cb2-a627-4276-8183-c2e6cd04db23"},"source":["mape(y_test, np.round(pred))"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.3656852575492794"]},"metadata":{},"execution_count":107}]},{"cell_type":"markdown","metadata":{"id":"S6taAb1V-9De"},"source":["LR results\n","- 2016-01_1000_10 first week -> ~52%\n","- 2016-01_1000_30 first_week -> ~40% \n","- 2016-01_40_10 -> ~14%\n","- 2016-02_500_30 -> ~30%\n","- 2016-02_1000_30 -> ~38.6%\n","- 2016-02_250_30 -> ~23.7%\n","- 2016-02_125_30 -> ~19%\n","- 2016-02_40_10 -> ~13.3%\n","- 2016-02_125_10 -> ~21.3%"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gxB7ndnpKAjg","executionInfo":{"status":"ok","timestamp":1634399409289,"user_tz":-240,"elapsed":5367,"user":{"displayName":"AI Vengers","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06452332470317445956"}},"outputId":"d9f5deb8-9230-4da5-dc2c-8f3b6156241a"},"source":["xg_model = xgb.XGBRegressor(\n"," learning_rate =0.1,\n"," n_estimators=600,\n"," max_depth=2,\n"," min_child_weight=3,\n"," gamma=0,\n"," subsample=0.8,\n"," reg_alpha=200, reg_lambda=200,\n"," colsample_bytree=0.8,nthread=4)\n","xg_model.fit(x_train, y_train)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[15:50:02] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"]},{"output_type":"execute_result","data":{"text/plain":["XGBRegressor(colsample_bytree=0.8, max_depth=2, min_child_weight=3,\n","             n_estimators=600, nthread=4, reg_alpha=200, reg_lambda=200,\n","             subsample=0.8)"]},"metadata":{},"execution_count":140}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"T1XL0edOPcNg","executionInfo":{"status":"ok","timestamp":1634399528849,"user_tz":-240,"elapsed":388,"user":{"displayName":"AI Vengers","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06452332470317445956"}},"outputId":"f0e5e9c6-38a7-481a-e253-42b034a12d2f"},"source":["pred = xg_model.predict(x_test)\n","mape(y_test, np.round(pred))"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.1400446824136367"]},"metadata":{},"execution_count":144}]},{"cell_type":"code","metadata":{"id":"7cI35kw5P8mc"},"source":["from sklearn.ensemble import RandomForestClassifier"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PfYxJuUhQMeI","executionInfo":{"status":"ok","timestamp":1634399535023,"user_tz":-240,"elapsed":1825,"user":{"displayName":"AI Vengers","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06452332470317445956"}},"outputId":"3c37466b-1f67-4fe1-89a3-8a91fbe3a00c"},"source":["regr1 = RandomForestRegressor(max_features='sqrt',min_samples_leaf=9,min_samples_split=7,n_estimators=79, n_jobs=-1)\n","regr1.fit(x_train, y_train)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["RandomForestRegressor(max_features='sqrt', min_samples_leaf=9,\n","                      min_samples_split=7, n_estimators=79, n_jobs=-1)"]},"metadata":{},"execution_count":145}]},{"cell_type":"code","metadata":{"id":"s8CIhkV4QTU7"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wBOsv6KuQckI"},"source":["def save_prediction(data, year, month, num_clusters, bin_mins, save_dir=\"/content/gdrive/MyDrive/New_York_Data/Predicted/\"):\n","  name = get_name(year, month, num_clusters, bin_mins)\n","  save_path = os.path.join(save_dir, name)\n","  if os.path.exists(save_path):\n","    print(\"{} already exists, skipping...\".format(save_path))\n","    return save_path\n","  with open(save_path, \"wb\") as f:\n","    np.save(f, data)\n","  print(data.shape)\n","  return save_path\n","\n","def save_to_json(data, year, month, num_clusters, bin_mins, save_dir=\"/content/gdrive/MyDrive/New_York_Data/Predicted/\"):\n","  name = get_name(year, month, num_clusters, bin_mins)\n","  save_path = os.path.join(save_dir, name)\n","  if os.path.exists(save_path):\n","    print(\"{} already exists, skipping...\".format(save_path))\n","    return save_path\n","  with open(save_path, \"wb\") as f:\n","    np.save(f, data)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":53},"id":"Ex9zckeNMwC3","executionInfo":{"status":"ok","timestamp":1634425363911,"user_tz":-120,"elapsed":290,"user":{"displayName":"AI Vengers","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06452332470317445956"}},"outputId":"7dba077f-835f-4667-8d64-f2bce1407ab9"},"source":["pred = lr.predict(x_test[:num_clusters])\n","save_prediction(pred, year, month, num_clusters, bin_mins)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(1000,)\n"]},{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'/content/gdrive/MyDrive/New_York_Data/Predicted/2016-02_1000_60.npy'"]},"metadata":{},"execution_count":109}]},{"cell_type":"code","metadata":{"id":"iCb0IgzEM-w2"},"source":[""],"execution_count":null,"outputs":[]}]}